{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At any given moment, our minds concentrate on a subset of the total information available to them. This is important, because the field of sensation is wide, and the mind’s bandwidth to process information is narrow, and some inputs are indeed more important that others, with regard to any given goal.\n",
    "\n",
    "<img width=300 src=\"images/visual_attention.png\"/>\n",
    "<center><a href=\"http://proceedings.mlr.press/v37/xuc15.pdf\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Neural networks can achieve this same behavior using an attention mechanism, which distributes attention over several inputs or encoded hidden states. And just as importantly, it accords different weights, or degrees of importance, to those inputs which are highly correlated with the current output. \n",
    "- To make attention differentiable, the network has to focus on each input, just to different extents. \n",
    "- In neural networks, attention is a memory-access mechanism, where memory is an abstract layer defining the context for each incoming time step. Attention mechanisms are components of memory networks, which focus their attention on external memory storage.\n",
    "- Attention is used for machine translation, speech recognition, reasoning, image captioning, summarization, and the visual identification of objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-decoder approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The encoder should memorize this long sequence into one vector, and the decoder has to process this vector to generate the translation.\n",
    "- A potential issue with a traditional encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector (also known as sentence embedding or “thought” vector).\n",
    "- By utilizing the attention mechanism, it is possible for decoder to capture global information rather than solely to infer based on one hidden state.\n",
    "\n",
    "<img width=500 src=\"images/1*75Jb0q3sX1GDYmJSfl-gOw.gif\"/>\n",
    "<center><a href=\"https://medium.com/@umerfarooq_26378/neural-machine-translation-with-code-68c425044bbd\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- The encoder is a bidirectional RNN (or other recurrent network setting of your choice) with a forward and backward hidden states, both combined into one vector. The decoder is a RNN with a forward hidden state for the output word at some position. This hidden state is calculated based on the previous state and the context vector.\n",
    "\n",
    "<img width=300 src=\"images/0*Jpp6WALMjZbjUFjP.png\"/>\n",
    "<center><a href=\"https://hackernoon.com/attention-mechanism-in-neural-network-30aaf5e39512\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- The context vector is a sum of hidden states (also called annotations) of the input sequence, weighted by alignment scores, which define how much of each hidden state in encoder should be considered for each output in decoder.\n",
    "- The alignment scores are calculated for each pair of the encoded input and the previous output of the decoder, based on how well they match. This is done by a feedforward neural network that is trained with all the other components of the system.\n",
    "\n",
    "<img width=200 src=\"images/1*Agii69DmmkTAGBLNUCeN0g.png\"/>\n",
    "\n",
    "- The alignment scores are then normalized using a softmax function. So $\\alpha_{i,j}$ is the amount of attention $y_i$ should pay to $x_j$. Sum over the attention weights for each element in the input sequence should be 1.\n",
    "\n",
    "<img width=200 src=\"images/1*VBalT1KkZ16WGvjWpjYo4g.png\"/>\n",
    "\n",
    "- The matrix of alignment scores is a nice byproduct to explicitly show the correlation between source and target words.\n",
    "\n",
    "<img width=400 src=\"images/bahdanau-fig3.png\"/>\n",
    "<center><a href=\"https://arxiv.org/pdf/1508.04025.pdf\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Finally, the context vector becomes a weighted sum of the annotations and normalized alignment scores.\n",
    "\n",
    "<img width=200 src=\"images/1*uXp_uFfXbAqqrJx5g-xYzQ.png\"/>\n",
    "\n",
    "- Each decoder output now depends not just on the last decoder state, but on a weighted combination of all the input states. While the context vector has access to the entire input sequence, we don’t need to worry about forgetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits:\n",
    "- Attention overcomes the information bottleneck in CNNs and RNNs by accessing the whole input\n",
    "- Attention weights help us visualize what the model deems important.  We can render attention as a heat map over input data such as words and pixels, and thus communicate to human operators how a neural network made a decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks:\n",
    "- That advance, like many increases in accuracy, comes at the cost of increased computational demands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n",
    "- [Attention and Augmented Recurrent Neural Networks](https://distill.pub/2016/augmented-rnns/)\n",
    "- [Implementing NLP Attention Mechanisms with DeepLearning4J](https://www.youtube.com/watch?v=XrZ_Y4koV5A&feature=youtu.be&t=249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
