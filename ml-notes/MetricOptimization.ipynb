{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A metric is used to measure the algorithm's performance.\n",
    "- A loss function is used to optimize an algorithm. \n",
    "- If a metric is not optimizable:\n",
    "    - Preprocess train and optimize another metric\n",
    "    - Optimize another metric and postprocess predictions\n",
    "    - Write a custom loss function\n",
    "    - Optimize another metric and use early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSE, RMSE and R-squared are similar from the optimization perspective.\n",
    "- Do you have outliers (mistakes, measurement errors)? Use MAE\n",
    "- Do you have unexpected values you should care about? Use (R)MSE\n",
    "- MSPE and MAPE are sensitive to relative errors and can be thought as weighted versions of MSE and MAE, respectively. \n",
    "    - Relative errors are intended to be both independent of scale and usable on all scales.\n",
    "    - They are mainly used as forecast accuracy measures in time-series data.\n",
    "    - [A Survey of Forecast Error Measures](https://pdfs.semanticscholar.org/435e/b6e05fcda264c8c1e7fbcdee7116bb5b1424.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE-based metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE:\n",
    "<center><img width=250 src=\"images/hmZydSW9YegiMVPWq2JBpOpai3CejzQpGkNG.gif\"/></center>\n",
    "\n",
    "- Mean Squared Error (MSE) measures the average squared difference between the estimated values and what is estimated. \n",
    "- Can range from 0 to $\\infty$ and are indifferent to the direction of errors. \n",
    "- It is a negatively-oriented score, which means lower values are better.\n",
    "- Useful if there are any unexpected values that we should care about.\n",
    "- MSE is one of the most widely used loss functions because of its ability to partition the variation in a dataset into variation explained by the model and variation explained by randomness.\n",
    "- Limitations:\n",
    "    - The least useful metric because of the lack of interpretability.\n",
    "    - MSE has the disadvantage of heavily weighting outliers.\n",
    "    $$MSE(30, 20)=100$$\n",
    "    $$MSE(30000, 20000)=100000000$$\n",
    "    - Results in a particularly problematic behaviour if applied on noisy data: a single bad prediction may skew the metric towards underestimating the model's quality; if errors are smaller than one, the opposite effect takes place.\n",
    "- Best constant: mean\n",
    "- Loss function:\n",
    "    - MSE can be directly optimized and is implemented by most libraries.\n",
    "    - Synonyms: *L2 Loss*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE:\n",
    "<center><img width=300 src=\"images/rmse-2.png\"/></center>\n",
    "\n",
    "- Root Mean Square Error (RMSE) is introduced to make scale of the errors to be the same as the scale of targets.\n",
    "- Expresses average model prediction error in units of the variable of interest.\n",
    "- Every minimizer of MSE is also a minimizer for RMSE and vice versa, since the square root is an non-decreasing function.\n",
    "- Even though RMSE and MSE are similar in terms of optimization, they are not interchangeable for gradient-based methods: Travelling along RMSE gradient is equivalent to traveling along MSE gradient but with a different flowing rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (R)MSLE:\n",
    "<center><img width=400 src=\"images/msle_01.png\"/></center>\n",
    "\n",
    "- Mean Squared Logarithmic Error (MSLE) is MSE calculated on logarithmic scale.\n",
    "- The introduction of the logarithm makes MSLE only care about the relative difference between predictions and target.\n",
    "\n",
    "$$MSLE(30, 20)=0.02861$$\n",
    "$$MSLE(30000, 20000)=0.03100$$\n",
    "\n",
    "- Used in cases where the range of the target value is large (e.g., in forecasting).\n",
    "\n",
    "<center><img width=200 src=\"images/1m_small.gif.png\"/></center>\n",
    "\n",
    "- The targets are usually non-negative but can equal to 0 - add a tiny constant before applying log.\n",
    "- RMSLE is considered as a better metric than MAPE, since it is less biased towards smaller targets.\n",
    "- Best constant: weighted mean in log space (weights are values themselves)\n",
    "- Loss function:\n",
    "    - Transform target with $z_i=\\log{y_i+1}$\n",
    "    - Fit a model with MSE loss\n",
    "    - Transform predictions back with $\\hat{y_i}=\\exp{\\hat{z_i}}-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R squared:\n",
    "<center><img width=400 src=\"images/1*WCaWmRreXCQxLez4yYOy5w.png\"/></center>\n",
    "\n",
    "- The coefficient of determination, or $R^2$ measures how much the model is better than the naive MSE baseline.\n",
    "- The MSE baseline can be thought of as the MSE that the simplest possible model would get.\n",
    "- Has the advantage of being scale-free (values between $-\\infty$ and $1$).\n",
    "- Values outside the range can occur when the model fits the data worse than the baseline. \n",
    "- Limitations:\n",
    "    - R-squared cannot determine whether the coefficient estimates and predictions are biased.\n",
    "    - Low values are not inherently bad for harder-to-predict data while high values do not necessarily indicate that the model has a good fit.\n",
    "    - $R^2$ will never decrease as variables are added and will probably experience an increase due to randomness. The adjusted R-squared can solve this by increasing only if the new term improves the model more than would be expected by chance.\n",
    "- Best constant: mean\n",
    "- Loss function:\n",
    "    - RMSE or MSE loss should be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE-based metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE:\n",
    "<center><img width=300 src=\"images/mae-equation-2.png\"/></center>\n",
    "\n",
    "- MAE measures the average magnitude of the errors in a set of predictions, without considering their direction.\n",
    "- Expresses average model prediction error in units of the variable of interest. \n",
    "- Can range from 0 to $\\infty$ and are indifferent to the direction of errors. \n",
    "- It is a negatively-oriented score, which means lower values are better.\n",
    "- The individual differences are weighted equally.\n",
    "- MAE loss is useful if the training data is corrupted with outliers.\n",
    "- It is widely used in finance, where error 10 is exactly two times worse than error 5.\n",
    "- If the absolute value is not taken, the average error becomes the Mean Bias Error (MBE).\n",
    "- Best constant: median (more robust to outliers than mean)\n",
    "- Loss function:\n",
    "    - MAE can be directly optimized but is implemented by fewer libraries.\n",
    "    - Not differentable when predictions are equal to target (rare case)\n",
    "    - One big problem in using MAE loss (for neural nets especially) is that its gradient will be large even for small loss values. Here helps a dynamic learning rate which decreases as we move closer to the minima.\n",
    "    - MAE criterion is slower than MSE criterion\n",
    "    - Synonyms: *L1 Loss, Median regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE vs. RMSE:\n",
    "- RMSE has a tendency to be increasingly larger than MAE as the test sample size increases.\n",
    "- RMSE has the benefit of penalizing large errors more.\n",
    "- MAE is the most robust choice in respect to outliers.\n",
    "- MAE is shown to be an unbiased estimator while RMSE is a biased estimator.\n",
    "- Loss functions:\n",
    "    - L1 loss is more robust to outliers, but its derivatives are not continuous, making it inefficient to find the solution. \n",
    "    - L2 loss is sensitive to outliers, but gives a more stable and closed form solution (by setting its derivative to 0.)\n",
    "    - [Comparing the performance using L1 loss and L2 loss](http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/)\n",
    "    - As option: Huber loss combines good properties from both MSE and MAE.\n",
    "    - As another option: Log-Cosh loss has all the advantages of Huber loss, and it’s twice differentiable everywhere (which is more favorable for models using Newton’s method), unlike Huber loss.\n",
    "    <center><img width=400 src=\"images/vXMgz.png\"/></center>\n",
    "    <center><a href=\"https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPE:\n",
    "<center><img width=300 src=\"images/internet-banking-22-246-e012.png\"/></center>\n",
    "\n",
    "- Mean Absolute Percentage Error (MAPE) or Mean Absolute Deviation (MAD) is relative MAE.\n",
    "- MAPE is often preferred because apparently managers understand percentages better than squared errors.\n",
    "- It is also commonly used as a loss function for regression problems and in model evaluation because of easy interpretation.\n",
    "- MAPEs greater than 100% can occur.\n",
    "- Limitations:\n",
    "    - MAPE is biased towards smaller targets which yield higher errors.\n",
    "    - MAPE can't be used for values where divisions and ratios make no sense (temperature scales).\n",
    "    - RMSE method is more accurate and recent.\n",
    "- Best constant: weighted median\n",
    "- Loss function:\n",
    "    - Use weights for samples (`sample_weight`) and optimize MAE.\n",
    "    - Or resample the dataset beforehand (`df.sample`) and optimize MAE.\n",
    "    - Usually need to resample many times and average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logloss is the only metric that is easy to optimize directly.\n",
    "- For a binary classification task, fit any metric, and tune with the binarization threshold. \n",
    "- For multi-class tasks, fit any metric and tune parameters comparing the models by their accuracy score (not optimization metric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=300 src=\"images/accuracy_score.png\"/></center>\n",
    "\n",
    "- Accuracy is the ratio of number of correct predictions to the total number of input samples.\n",
    "- Works well for balanced datasets (one can get a nearly perfect score with imbalanced data classes).\n",
    "- To compute accuracy we need hard predictions, that is, apply a threshold.\n",
    "- The optimal threshold can be found with grid search.\n",
    "- Best constant: the most frequent class\n",
    "- Loss function:\n",
    "    - The loss function is not differentable since gradients are zero almost always.\n",
    "    - Zero-one loss may be approximated with a proxy loss such as logloss or Hinge loss.\n",
    "    <center><img width=400 src=\"images/RUIJ4.png\"/></center>\n",
    "    <center><a href=\"https://stackoverflow.com/questions/47716601/classification-modified-huber-loss-how-is-it-more-tolerant-to-outliers\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with imbalanced data:\n",
    "- Resampling:\n",
    "    - Undersampling: sample from the majority class\n",
    "    - Oversampling: replicate points from the minority class\n",
    "    - Consider testing random and non-random (e.g., stratified) sampling schemes.\n",
    "    - Consider testing different resampled ratios.\n",
    "- Generating synthetic data: create new synthetic points from the minority class or both classes (see SMOTE).\n",
    "- But information about class distribution is lost when resampling the dataset and can skew predictions on test.\n",
    "- Penalize mistakes on minority classes with class re-weighting (`sample_weight`).\n",
    "- Use a different performance metric (e.g., precision, recall, F-score, ROC AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=350 src=\"images/logloss.png\"/></center>\n",
    "\n",
    "- Logarithmic loss, or logloss, increases as the predicted probability diverges from the actual label.\n",
    "- For all samples, enforces the model to assign a probability to be of a certain class.\n",
    "- To ensure numeric stability, predictions are clipped to be from $[10^{-15}, 1-10^{-15}]$ instead of $[0, 1]$.\n",
    "- Logloss has no upper bound ($[0, \\infty)$) with smaller values indicating a better fit.\n",
    "- Best constant: class frequency as vector\n",
    "- Loss function:\n",
    "    - Logloss can be directly optimized and is implemented by most libraries.\n",
    "    - NNs at default optimize logloss for classification.\n",
    "    - Random forest classifiers are bad at logloss and produce rather conservative predictions. RFs can be calibrated in several ways: 1) Platt scaling (fit logistic regression to predictions), 2) isotonic regression, and 3) stacking.\n",
    "    - Calibrated classifiers (rarely the case) will return posterior probabilities where threshold 0.5 would be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Area under the ROC Curve (AUC) of a classifier is a performance measurement for classification problem at various thresholds settings.\n",
    "- AUC tells how much model is capable of distinguishing between classes 0 and 1.\n",
    "\n",
    "<center><img width=350 src=\"images/glucosedistn.png\"/></center>\n",
    "<center><a href=\"http://corysimon.github.io/articles/what-is-an-roc-curve/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "    \n",
    "- AUC is a rank-based metric (depends on ordering of predictions, not on absolute values)\n",
    "- Used for binary classification problems. \n",
    "    - For multi-class tasks, we can plot $N$ number of AUC-ROC curves for $N$ number classes using one-vs-all methodology.\n",
    "- Good for cases when you need to estimate how well your model is at discriminating TRUE from FALSE values.\n",
    "- Removes the need for hard predictions and dependency on the threshold.\n",
    "- AUC can be obtained in different ways:\n",
    "    - Area under the curve of plot FPR ($\\frac{TP}{TP+FN}$) vs TPR ($\\frac{FP}{FP+TN}$) at different points in $[0, 1]$ ([Understanding ROC curves](http://www.navan.name/roc/))\n",
    "    - The probability of a pair of the predictions to be ordered in the right way.\n",
    "    - The expectation that the classifier will rank a randomly chosen positive higher than a randomly chosen negative.\n",
    "- Best constant: any constant, random predictions lead to $AUC=0.5$\n",
    "- Loss function:\n",
    "    - Although the loss function of AUC has zero gradients almost everywhere, exactly as accuracy loss, there exists an algorithm to optimize AUC with gradient-based methods, which use pair-wise loss instead of point-wise loss (supported by LGBM, XGBoost).\n",
    "    - XGBoost learned with logloss gives a comparable AUC score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=250 src=\"images/b1bd8c24-0675-4070-b605-acd937161cf9.png\"/></center>\n",
    "\n",
    "- The kappa statistic is a metric that compares an observed accuracy with an expected accuracy (random chance). An accuracy of 80% is a lot more impressive with an expected accuracy of 50% versus an expected accuracy of 75%.\n",
    "- It is used not only to evaluate a single classifier, but also to evaluate classifiers amongst themselves.\n",
    "- Outputs 0 for baseline accuracy and 1 for excelent accuracy.\n",
    "- The more complex (and random) the dataset is, the lower kappa will be considered as high enough (such as 0.4)\n",
    "- Similar to how R squared better scales the MSE values for being easier explained.\n",
    "- Kappa is quite intuitive for medicine applications: how much the model agrees with professional doctors?\n",
    "- Best constant: the most frequent class\n",
    "- Loss function:\n",
    "    - Treat this task as a regression problem (but more complex than MSE) and post-process the predictions by rounding them with a (tuned) threshold (if we relax the predictions to take values between the labels).\n",
    "    - [Or implement \"soft kappa\" loss](https://arxiv.org/abs/1509.07107)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Learning to Rank using Gradient Descent](http://icml.cc/2015/wp-content/uploads/2015/06/icml_ranking.pdf) -- original paper about pairwise method for AUC optimization\n",
    "- [Overview of further developments of RankNet](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/MSR-TR-2010-82.pdf)\n",
    "- [RankLib (implementations for the 2 papers from above)](https://sourceforge.net/p/lemur/wiki/RankLib/)\n",
    "- [Learning to Rank Overview](https://wellecks.wordpress.com/2015/01/15/learning-to-rank-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Evaluation metrics for clustering](http://nlp.uned.es/docs/amigo2007a.pdf)\n",
    "- [Unsupervised learning: PCA and clustering](https://www.kaggle.com/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
