{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An ensemble method combines the predictions of many individual classifiers by majority voting.\n",
    "- Ensemble of *low-correlating* classifiers with slightly greater than 50% accuracy will outperform each of the classifiers individually.\n",
    "- Condorcet's jury theorem: \n",
    "    - If each member of the jury (of size $N$) makes an *independent* judgement and the probability $p$ of the correct decision by each juror is more than 0.5, then the probability of the correct decision $P_N$ by the majority $m$ tends to one. On the other hand, if $p<0.5$ for each juror, then the probability tends to zero.\n",
    "    <center><img width=350 src=\"images/Screen Shot 2019-06-06 at 20.29.13.png\"/></center>\n",
    "    - where $m$ as a minimal number of jurors that would make a majority.\n",
    "    - But real votes are not independent, and do not have uniform probabilities.\n",
    "- Uncorrelated submissions clearly do better when ensembled than correlated submissions.\n",
    "- Majority votes make most sense when the evaluation metric requires hard predictions.\n",
    "- Choose bagging for base models with high variance.\n",
    "- Choose boosting for base models with high bias.\n",
    "- Use averaging, voting or rank averaging on manually-selected well-performing ensembles.\n",
    "- [KAGGLE ENSEMBLING GUIDE](https://mlwave.com/kaggle-ensembling-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Averaging is taking the mean of individual model predictions.\n",
    "- Averaging predictions often reduces variance (as bagging does).\n",
    "- It’s a fairly trivial technique that results in easy, sizeable performance improvements.\n",
    "- Averaging exactly the same linear regressions won't give any penalty.\n",
    "- An often heard shorthand for this on Kaggle is \"bagging submissions\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted averaging:\n",
    "- Use weighted averaging to give a better model more weight in a vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional averaging:\n",
    "- Use conditional averaging to cancel out erroneous ranges of individual estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging (bootstrap aggregating) considers *homogeneous* models, learns them independently from each other in parallel, and combines them following some kind of deterministic averaging process.\n",
    "- Bagging combines *strong learners* together in order to \"smooth out\" their predictions and reduce variance.\n",
    "- Bootstrapping allows to fit models that are roughly independent.\n",
    "\n",
    "<center><img width=350 src=\"images/Ozone.png\"/></center>\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/Bootstrap_aggregating\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- The procedure is as follows:\n",
    "    - Create $N$ random sub-samples (with replacement) for the dataset of size $N$.\n",
    "    - Fit a base model on each sample.\n",
    "    - Average predictions from all models.\n",
    "- Can be used with any type of method as a base model.\n",
    "- Bagging is effective on small datasets.\n",
    "- Out-of-bag estimate is the mean estimate of the base algorithms on 37% of inputs that are left out of a particular bootstrap sample.\n",
    "    - Helps avoid the need for an independent validation dataset.\n",
    "- Parameters to consider:\n",
    "    - Random seed\n",
    "    - Row sampling or bootstrapping\n",
    "    - Column sampling or bootstrapping\n",
    "    - Size of sample (use a much smaller sample size on a larger dataset)\n",
    "    - Shuffling\n",
    "    - Number of bags\n",
    "    - Parallelism\n",
    "- See [Tree-Based Models](https://nbviewer.jupyter.org/github/polakowo/machine-learning/blob/master/ml-notes/TreeBasedModels.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping:\n",
    "- Bootstrapping is random sampling with replacement.\n",
    "- With sampling with replacement, each sample unit has an equal probability of being selected.\n",
    "    - Samples become approximatively independent and identically distributed (i.i.d).\n",
    "    - It is a convenient way to treat a sample like a population.\n",
    "- This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.\n",
    "- It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators.\n",
    "- For example:\n",
    "    - Select a random element from the original sample of size $N$ and do this $B$ times.\n",
    "    - Calculate the mean of each sub-sample.\n",
    "    - Obtain a 95% confidence interval around the mean estimate for the original sample.\n",
    "- Two important assumptions:\n",
    "    - $N$ should be large enough to capture most of the complexity of the underlying distribution (representativity). \n",
    "    - $N$ should be large enough compared to $B$ so that samples are not too much correlated (independence).\n",
    "- An average bootstrap sample contains 63.2% of the original observations and omits 36.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Boosting considers *homogeneous* models, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy.\n",
    "- This technique is called boosting because we expect an ensemble to work much better than a single estimator.\n",
    "- Sequential methods are no longer fitted independently from each others and can't be performed in parallel.\n",
    "- Each new model in the ensemble focuses its efforts on the most difficult observations to fit up to now.\n",
    "- Boosting combines weak learners together in order to create a strong learner with lower bias.\n",
    "    - A weak learner is defined as one whose performance is at least slightly better than random chance.\n",
    "    - These learners are also in general less computationally expensive to fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive boosting:\n",
    "- At each iteration, adaptive boosting changes the sample distribution by modifying the weights of instances. \n",
    "    - It increases the weights of the wrongly predicted instances. \n",
    "    - The weak learner thus focuses more on the difficult instances. \n",
    "- The procedure is as follows:\n",
    "    - Fit a weak learner $h_t$ with the current observations weights.\n",
    "    - Estimate the learner's performance and compute its weight $\\alpha_t$ (contribution to the ensemble).\n",
    "    - Update the strong learner by adding the new weak learner multiplied by its weight.\n",
    "    - Compute new observations weights that expresse which observations to focus on.\n",
    "<center><img width=300 src=\"images/adaboost_finalclassifier.png\"/></center>\n",
    "- See [Tree-Based Models](https://nbviewer.jupyter.org/github/polakowo/machine-learning/blob/master/ml-notes/TreeBasedModels.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boosting:\n",
    "- Gradient boosting doesn’t modify the sample distribution:\n",
    "    - At each iteration, the weak learner trains on the remaining errors (so-called pseudo-residuals) of the strong learner.\n",
    "- Gradient boosting doesn’t weight weak learnes according to their performance:\n",
    "    - The contribution of the weak learner (so-called multiplier) to the strong one is computed using gradient descent. \n",
    "    - The computed contribution is the one minimizing the overall error of the strong learner.\n",
    "- Allows optimization of an arbitrary differentiable loss function.\n",
    "- The procedure is as follows:\n",
    "    - Compute pseudo-residuals that indicate, for each observation, in which direction we would like to move.\n",
    "    - Fit a weak learner $h_t$ to the pseudo-residuals (negative gradient of the loss)\n",
    "    - Add the predictions of $h_t$ multiplied by the step size $\\alpha$ (learning rate) to the predictions of ensemble $H_{t-1}$.\n",
    "<center><img width=300 src=\"images/Screen Shot 2019-06-08 at 10.34.25.png\"/></center>\n",
    "- See [Tree-Based Models](https://nbviewer.jupyter.org/github/polakowo/machine-learning/blob/master/ml-notes/TreeBasedModels.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stacking considers *heterogeneous* models, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions.\n",
    "- Stacking on a small holdout set is blending.\n",
    "- Stacking with linear regression is sometimes the most effective way of stacking.\n",
    "- Non-linear stacking gives surprising gains as it finds useful interactions between the original and the meta-model features.\n",
    "- Feature-weighted linear stacking stacks engineered meta-features together with model predictions.\n",
    "- At the end of the day you don’t know which base models will be helpful.\n",
    "- Stacking allows you to use classifiers for regression problems and vice versa.\n",
    "- Base models should be as diverse as possible:\n",
    "    - 2-3 GBMs (one with low depth, one with medium and one with high)\n",
    "    - 2-3 NNs (one deeper, one shallower)\n",
    "    - 1-2 ExtraTrees/RFs (again as diverse as possible)\n",
    "    - 1-2 linear models such as logistic/ridge regression\n",
    "    - 1 kNN model\n",
    "    - 1 factorization machine\n",
    "- Use different features for different models.\n",
    "- Use feature engineering:\n",
    "    - Pairwise distances between meta features\n",
    "    - Row-wise statistics (like mean)\n",
    "    - Standard feature selection techniques\n",
    "- Meta models can be shallow:\n",
    "    - GBMs with small depth (2-3)\n",
    "    - Linear models with high regularization\n",
    "    - ExtraTrees\n",
    "    - Shallow NNs (1 hidden layer)\n",
    "    - kNN with BrayCurtis distance\n",
    "    - A simple weighted average (find weights with bruteforce)\n",
    "- Use automated stacking for complex cases to optimize:\n",
    "    - CV-scores\n",
    "    - Standard deviation of the CV-scores (a smaller deviation is a safer choice)\n",
    "    - Complexity/memory usage and running times\n",
    "    - Correlation (uncorrelated model predictions are preferred).\n",
    "- Greedy forward model selection:\n",
    "    - Start with a base ensemble of 3 or so good models.\n",
    "    - Add a model when it increases the train set score the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-level stacking:\n",
    "- Always do OOF predictions: you never know when you need to train a 2nd or 3rd level meta-classifier.\n",
    "- Try skip connections to deeper layers.\n",
    "- For 7.5 models in previous layer add 1 meta model in next layer.\n",
    "- Try [StackNet](https://github.com/h2oai/pystacknet) which resembles a feedforward neural network and uses Wolpert's stacked generalization (built iteratively one layer at a time) in multiple levels to improve accuracy  in machine learning problems.\n",
    "\n",
    "<center><img width=600 src=\"images/stacknet.png\"/></center>\n",
    "<center><a href=\"https://opendatascience.com/predicting-resignation-in-the-military/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Keep in mind:\n",
    "    - Adding levels can either be data expensive or time expensive.\n",
    "    - We cannot use backpropagation since not all models are differentiable.\n",
    "    - Performance plateauing after some number of models.\n",
    "    - Be mindful of target leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
