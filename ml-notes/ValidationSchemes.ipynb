{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Never use data you train on to measure the quality of your model (resubstitution).\n",
    "- Hold out part of the available data as a test set.\n",
    "    - There is still a risk of overfitting because the parameters can be tweaked until the estimator performs optimally.\n",
    "    - Only the final evaluation should be done on the test set.\n",
    "- Set up validation to mimic train/test split.\n",
    "    - In a competition, you need to identify the train/test split made by organizers.\n",
    "    - In most cases, data is split by rows, time, groups or combined.\n",
    "    - Logic of feature engineering depends on the data splitting strategy.\n",
    "    \n",
    "<center><img width=400 src=\"images/grid_search_workflow.png\"/></center>\n",
    "<center><a href=\"https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation stage:\n",
    "- We can observe that:\n",
    "    - High deviation of the local CV scores.\n",
    "- Possible causes:\n",
    "    - Too little data.\n",
    "    - Data is too diverse and inconsistent (e.g., December and January for store sales).\n",
    "- Extensive validation techniques:\n",
    "    - Perform k-fold split multiple times with different seeds, and average the scores.\n",
    "    - Tune a model on one split, evaluate the model on the other.\n",
    "- Following problems can be identified before the submission stage:\n",
    "    - Different scores/optimal parameters between folds.\n",
    "    - Public leaderboard score will be unreliable because of too little data.\n",
    "    - Train and test data are from different distributions (using EDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission stage:\n",
    "- We can observe that:\n",
    "    - LB score is consistently lower/higher than the local score:\n",
    "    - LB score is not correlated with the local score.\n",
    "- Possible causes:\n",
    "    - We may already have a high variance of CV scores: calculate mean and std of CV scores and estimate if LB is expected.\n",
    "    - Too little data in public leaderboard: trust your local validation.\n",
    "    - Train and test are from different distributions: adjust distributions or perform leaderboard probing.\n",
    "    - Overfitting\n",
    "    - Incorrect cross-validation strategy\n",
    "- Expect LB shuffle because of:\n",
    "    - Randomness can shuffle scores on the private leaderboard.\n",
    "    - Little amount of training or/and testing data.\n",
    "    - Different public/private data or target distributions (e.g., time-based split).\n",
    "- [How to Select Your Final Models in a Kaggle Competition](http://www.chioka.in/how-to-select-your-final-models-in-a-kaggle-competitio/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of base models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following validation schemes are supposed to be used to estimate quality of the model. \n",
    "- For getting test predictions don't forget to retrain your model using all training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holdout:\n",
    "- Procedure (`train_test_split`):\n",
    "    - Split *train* into two parts: *trainA* and *trainB* (usually 80/20).\n",
    "    - Fit the model on *trainA* and validate it on *trainB*.\n",
    "- Use holdout if scores on each fold are roughly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold:\n",
    "- Procedure (`KFold`):\n",
    "    - Split *train* into $K$ folds.\n",
    "    - Iterate though each fold: \n",
    "        - Refit the model on all folds except the current one.\n",
    "        - Validate the model on the current fold.\n",
    "- Assumes that the samples are i.i.d.\n",
    "- The performance measure reported by k-fold CV is the average of the values computed in the loop.\n",
    "- Scores deviation in KFold can help to select statistically significant change in scores while tuning a model.\n",
    "- The value of $K$ being large could lead to low bias and high variance (overfitting).\n",
    "- The advantage is that entire data is used for training and validation.\n",
    "\n",
    "<center><img width=500 src=\"images/sphx_glr_plot_cv_indices_0041.png\"/></center>\n",
    "<center><a href=\"https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- For classification problems that exhibit a large imbalance (`StratifiedKFold`):\n",
    "    - Stratified k-fold ensures that relative class frequencies are preserved in each train and validation fold.\n",
    "- In this case we would like to know if a model generalizes well to the unseen groups (`GroupKFold`):\n",
    "    - Group k-fold ensures that the same group is not represented in both testing and training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOO:\n",
    "- LOO is a k-fold scheme where $K=N$ (`LeaveOneOut`).\n",
    "- LOO often results in high variance as an estimator for the test error.\n",
    "- 5-10 fold cross validation should be preferred to LOO.\n",
    "- Mostly used for sparse datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time-based validation:\n",
    "- Procedure (`TimeSeriesSplit`):\n",
    "    - Split *train* into chunks of duration $T$. Select first $M$ chunks.\n",
    "    - Fit a model on these $M$ chunks and predict for the chunk $M+1$. \n",
    "    - Then repeat this procedure for the next chunk and so on (imagine a moving window).\n",
    "- Used if the samples have been generated using a time-dependent process.\n",
    "- Time series data is characterised by the correlation between observations (autocorrelation).\n",
    "- Does not assume that the samples are i.i.d.\n",
    "\n",
    "<center><img width=450 src=\"images/Q37Bn.png\"/></center>\n",
    "<center><a href=\"https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of meta models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple holdout scheme:\n",
    "- Procedure:\n",
    "    - Split *train* into three parts: *trainA*, *trainB* and *trainC*.\n",
    "    - Fit $N$ diverse models on *trainA*.\n",
    "    - Predict for *trainB*, *trainC*, and *test* (getting meta-features *trainB_meta*, *trainC_meta* and *test_meta*).\n",
    "    - Fit a meta-model on *trainB_meta* and validate it on *trainC_meta*.\n",
    "    - When the meta-model is validated, fit it to *[trainB_meta, trainC_meta]* and predict for *test_meta*.\n",
    "- This scheme is usually preferred over the other schemes if dataset is large.\n",
    "- Fair validation scheme (validation set of meta-models not used in any way by base models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta holdout scheme with OOF meta-features:\n",
    "- Procedure:\n",
    "    - Split *train* into $K$ folds. \n",
    "    - Iterate though each fold:\n",
    "        - Refit $N$ diverse models on all folds except the current one.\n",
    "        - Predict for the current fold. \n",
    "        - For each object in *train*, we now have $N$ meta-features (out-of-fold predictions, OOF) (getting *train_meta*)\n",
    "    - Fit the models on *train* and predict for *test* (getting *test_meta*)\n",
    "    - Split *train_meta* into two parts: *train_metaA* and *train_metaB*. \n",
    "    - Fit a meta-model on *train_metaA* and validate it on *train_metaB*.\n",
    "    - When the meta-model is validated, fit it to *train_meta* and predict for *test_meta*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meta KFold scheme with OOF meta-features:\n",
    "- Procedure:\n",
    "    - Obtain OOF predictions for *train_meta* and *test_meta*.\n",
    "    - Use KFold scheme on *train_meta* to validate the meta-model (with same seed as for OOF).\n",
    "    - When the meta-model is validated, fit it to *train_meta* and predict for *test_meta*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holdout scheme with OOF meta-features:\n",
    "- Procedure:\n",
    "    - Split *train* into two parts: *trainA* and *trainB*.\n",
    "    - Fit models to *trainA* and predict for *trainB* (getting *trainB_meta*).\n",
    "    - Obtain OOF predictions for *trainA_meta*.\n",
    "    - Fit a meta-model to *trainA_meta* and validate on *trainB_meta*.\n",
    "    - Obtain OOF predictions for *train_meta* and *test_meta*.\n",
    "    - Fit the meta-model to *train_meta* and predict for *test_meta*.\n",
    "- Fair validation scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  KFold scheme with OOF meta-features:\n",
    "- The same as holdout scheme with OOF meta-features but with $K$ folds instead of *trainA* and *trainB*.\n",
    "- This scheme gives the validation score with the least variance compared to the other schemes.\n",
    "- But it is also the least efficient one from the computational perspective.\n",
    "- Fair validation scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold scheme in time series:\n",
    "- Procedure:\n",
    "    - Obtain OOF meta-features using time-series split starting with $M$ chunks.\n",
    "    - Now we have meta-features for the chunks starting from $M+1$ (getting *train_meta*).\n",
    "    - Fit the models on *train* and predict for *test* (getting *test_meta*).\n",
    "    - Perform time-series aware cross validation on meta-features.\n",
    "    - Fit the meta-model to *train_meta* and predict for *test_meta*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KFold scheme in time series with limited amount of data:\n",
    "- The same as meta KFold scheme with OOF meta-features but with respect to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
