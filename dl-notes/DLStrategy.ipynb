{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Machine learning strategy is useful to iterate through ideas quickly and to efficiently reach the project outcome.\n",
    "- Build your first system quickly, then iterate: Quickly prototype a first version of the classifier and then improve it iteratively following the strategic guidelines.\n",
    "- Respect orthogonalization: Refers to the concept of picking parameters to tune which only adjust one outcome of the machine learning model, e.g. regularization is a knob to reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metric:\n",
    "- To choose a classifier, a well-defined development set and an evaluation metric speed up the iteration process.\n",
    "- Pick one evaluation metric, e.g. F-score, to instantly judge the performance of multiple models.\n",
    "- When to change dev/test sets and metrics: If you find out that the rank of your evaluation metric doesn’t accurately reflect the performance of your models anymore, consider restating the optimization metric, e.g. through adding a weighting term to heavily penalize your classifier for misclassifying really important examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Satisficing and optimizing metrics:\n",
    "- A machine learning model generally has one metric to optimize for, e.g. achieve maximum accuracy, and certain constraints which should be upheld, e.g. calculate predictions in less than 1s or fit the model into local memory. In this case, accuracy is the optimizing metric and prediction time and memory usage are satisficing metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/dev/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training set: We train the model on the training data.\n",
    "- Validation set: After training the model, we validate it on the dev set.\n",
    "- Test set: When we have a final model (i.e., the model that has performed well on both training as well as dev set), we evaluate it on the test set in order to get an unbiased estimate of how well our algorithm is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions:\n",
    "- Make sure that the dev/test sets come from the same distribution.\n",
    "- Make sure that the dev/test sets represent the target accurately that team tries to optimize for.\n",
    "- Size of the dev and test sets: Use as much data as possible for the training set and use 1%/1% for the dev/test sets, given that your training set is in the millions.\n",
    "- Divide the training, dev and test sets in such a way that their distribution is similar.\n",
    "- Training and testing on different distributions: If your data in the training set comes from mixed data sources, create the dev and test sets with the data that you want to optimize for, e.g. if you want to classify sneaker images from a phone, use a dev and test set consisting only of sneaker photos from mobile phones but feel free to use enhanced sneaker web images to train the network.\n",
    "- Bias and variance with mismatched data: Create a training-dev set with the same data distribution as the training set when you have a dev and test set from a different data distribution. This step helps you check if you have a variance, bias or data mismatch problem.\n",
    "\n",
    "<img width=500 src=\"images/im31.png\"/>\n",
    "<center><a href=\"https://yashuseth.blog/2018/03/20/what-to-do-when-we-have-mismatched-training-and-validation-set/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without data mismatch | With data mismatch\n",
    ":-:|:-:\n",
    "<img width=400 src=\"images/1*-PJMjoc3sPv5LZGCbXFyMg.png\"/>  |  <img width=600 src=\"images/errors.png\"/>\n",
    "\n",
    "<center><a href=\"https://medium.com/machine-learning-bites/deeplearning-series-how-to-structure-machine-learning-projects-ae484c0919c3\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bayes error is the best performance that a classifier can achieve and by definition better than human-level performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-level error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Human-level error is important metric to evaluate whether your training data suffers from bias.\n",
    "- If a group of experts is able to achieve an error rate of 0.7% and a single human achieves 1% error rate, chose 0.7% as the best human-level performance and a value <0.7% as the Bayes error to test model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surpassing human-level performance:\n",
    "- In the basic setting, DL models tend to plateau once they have reached or surpassed human-level accuracy. \n",
    "- Human-level performance can serve as a very reliable proxy which can be leveraged to determine your next move when training your model.\n",
    "- If your algorithm surpasses human-level performance, it becomes very hard to judge the avoidable bias because you generally don’t know how small the Bayes error is.\n",
    "\n",
    "<img width=500 src=\"images/1*iSygwQMVlGpyRofod_iotg.png\"/>\n",
    "<center><a href=\"https://towardsdatascience.com/how-to-improve-my-ml-algorithm-lessons-from-andrew-ngs-experience-ii-f66926926f88\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training error (Bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High bias means undefitting to the training set\n",
    "- When tackling a machine learning project, the first thing we want is good performance on the training set\n",
    "- Avoidable bias: Describes the gap between training set error and human-level performance.\n",
    "- Evaluate the difference between Bayes error and training set error to estimate the level of avoidable bias.\n",
    "- How to solve:\n",
    "    - Train a bigger model\n",
    "    - Train longer\n",
    "    - Use better optimization algorithms (Momentum, Adam, RMSprop)\n",
    "    - New architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-dev error (Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High variance means overfitting to the training set\n",
    "- How to solve:\n",
    "    - Gather more data\n",
    "    - Use regularization (L2, dropout, data augmentation)\n",
    "    - New architecture\n",
    "\n",
    "Bias vs Variance | Bias-Variance Tradeoff\n",
    ":-:|:-:\n",
    "<img width=400 src=\"images/Bias vs Variance.png\"/>  |  <img width=400 src=\"images/Bias-Variance-Tradeoff-660x445.png\"/>\n",
    "<center><a href=\"https://elitedatascience.com/bias-variance-tradeoff\" style=\"color: lightgrey\">Credit</a></center> | <center><a href=\"http://www.luigifreda.com/2017/03/22/bias-variance-tradeoff/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev error (Data mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Techniques:\n",
    "- If you have a data mismatch problem, carry out manual error analysis and understand the difference between training and dev/test sets. \n",
    "- Be mindful of creating artificial training data, because it could happen that you synthesize only a small subset of all available noise.\n",
    "- Data synthesis: Andrew also stressed the importance of data synthesis as part of any workflow in deep learning. While it may be painful to manually engineer training examples, the relative gain in performance you obtain once the parameters and the model fit well are huge and worth your while.\n",
    "- New architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error analysis:\n",
    "- Analyze 100 misclassifies examples and batch them by reason for misclassification. \n",
    "- To improve your model, it might make sense to train your network to eliminate the reason why it misclassifies a certain type of input, e.g. feed it with more foggy pictures.\n",
    "- Cleaning up incorrectly labeled data: Neural networks are pretty stable to handle random misclassifications and if you eliminate misclassifications in the dev set, also eliminate it in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Degree of overfitting to dev set\n",
    "- How to solve:\n",
    "    - More dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different aspects of learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are various deep learning networks with state-of-the-art performance that have been developed and tested across domains such as computer vision and natural language processing (NLP).\n",
    "- Two most popular strategies:\n",
    "    - Pre-trained models as feature extractors: \n",
    "        - The layered architecture allows us to utilize a pre-trained network (such as Inception V3 or VGG) without its final layer as a fixed feature extractor for other tasks.\n",
    "    - Fine-tuning: \n",
    "        - We do not just replace the final layer but also selectively retrain some of the previous layers of the base model. \n",
    "        - Satellite imagery and medical imagery, for example, require more lower-level fine-tuning.\n",
    "- In general, we can set learning rates to be different for each layer to find a tradeoff between freezing and fine-tuning.\n",
    "\n",
    "<img width=500 src=\"images/1*f2_PnaPgA9iC5bpQaTroRw.png\"/>\n",
    "<center><a href=\"https://medium.com/@subodh.malgonde/transfer-learning-using-tensorflow-52a4f6bcde3e\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)\n",
    "- [Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitask learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a single neural network to detect multiple classes in an image, e.g. traffic lights and pedestrians for an autonomous car. \n",
    "- Again, it is useful when the neural network identifies lower-level features which are helpful for multiple classification tasks and if you have an equal distribution of class data.\n",
    "\n",
    "<img width=600 src=\"images/1*RXWO8pWJelvFJrGEr8sRrg.png\"/>\n",
    "<center><a href=\"https://blog.manash.me/multi-task-learning-in-keras-implementation-of-multi-task-classification-loss-f1d42da5c3f6\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-end deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of using many different steps and manual feature engineering to generate a prediction, use one neural network to figure out the underlying pattern\n",
    "-  End-to-end deep learning has advantages like letting the network figure out important features itself and disadvantages like requiring lots of data, so its use really has to be judged on a case-by-case basis by how complex the task or function is that you are solving.\n",
    "\n",
    "<img width=500 src=\"images/deep-learning_W640.jpg\"/>\n",
    "<center><a href=\"https://www.researchgate.net/publication/322325843_Deep_learning_for_smart_manufacturing_Methods_and_applications/figures?lo=1&utm_source=google&utm_medium=organic\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\n",
    "- In order for ensemble methods to be more accurate than any of its individual members, the base learners have to be as accurate as possible and as diverse as possible.\n",
    "- [Ensemble Learning to Improve Machine Learning Results](https://blog.statsbot.co/ensemble-learning-d1dcd548e936)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snapshot ensembles:\n",
    "- One of the most effective methods is to train a single neural network, converging to several local minima along its optimization path, and save the model parameters. This way, we obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost.\n",
    "- [Snapshot Ensembles: Train 1, get M for free](https://arxiv.org/abs/1704.00109)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-time augmentation (TTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TTA is a form of data augmentation that a model uses during test time, as opposed to most data augmentation techniques that run during training time.\n",
    "- The technique works as follows:\n",
    "  - augment a test image in multiple ways\n",
    "  - use the model to classify these variants of the test image\n",
    "  - average the results of the model’s many predictions\n",
    "- The technique found popularity among some competitors in the ImageNet Large Scale Visual Recognition Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
