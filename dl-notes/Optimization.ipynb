{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The learning rate determines how quickly or how slowly you want to update the weights (or parameters). \n",
    "- Learning rate is one of the most difficult parameters to set, because it significantly affects model performance. The optimal learning rate is dependent on the topology of the loss landscape, which is in turn depends on both the model architecture and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divergence:\n",
    "- It's a key differentiator between convergence and divergence.\n",
    "- If learning rate is set too low, training will progress very slowly as you are making very tiny updates to the weights in the network.\n",
    "- Divergence: However, if learning rate is set too high, it can cause undesirable divergent behavior in the loss function.\n",
    "\n",
    "<img width=350 src=\"images/figure3.png\"/>\n",
    "<center><a href=\"http://www.cs.cornell.edu/courses/cs4780/2015fa/web/lecturenotes/lecturenote07.html\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local minima:\n",
    "- The best learning rate is associated with the steepest drop in loss.\n",
    "- Recent research has shown that local minima is not neccasarily bad. In the loss landscape of a neural network, there are just way too many minima, and a \"good\" local minima might perform just as well as a global minima.\n",
    "- A desirable property of a minima should be it that it should be on the flatter side, because flat minimum are easy to converge to, given there's less chance to overshoot the minima, and be bouncing between the ridges of the minima.\n",
    "\n",
    "<img width=250 src=\"images/noshort.png\"/>\n",
    "<center><a href=\"https://www.cs.umd.edu/~tomg/projects/landscapes/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saddle points:\n",
    "- Another key challenge of minimizing highly non-convex error functions is avoiding getting trapped in their numerous suboptimal local minima.\n",
    "- Dauphin et al. [3] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down.\n",
    "\n",
    "<img width=300 src=\"images/0*Q_ZjKKXa9mTShbpV.png\"/>\n",
    "<center><a href=\"https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.\n",
    "- Solution: Change the learning rate every iteration according to some (cyclic, validation, etc.) function\n",
    "- [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/pdf/1506.01186.pdf)\n",
    "- A simple decay is not often used for training deep neural networks. Instead, one uses per-parameter decay rates such as RMSProp or Adam or Adamax in order to prevent parameters from languishing on cost plateaus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent is used while training a machine learning model to minimize the cost function.\n",
    "- It is an optimization algorithm, based on a convex function, that tweaks it’s parameters iteratively to minimize a given function to its local minimum.\n",
    "- Using calculus, we know that the slope of a function is the derivative of the function with respect to a value. This slope always points to the nearest minimum. \n",
    "\n",
    "<img width=250 src=\"images/0*zHKyPhSCgI8wOwpU.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cons:\n",
    "- The objective function might be discontinuous\n",
    "- Gradient-based method are very efficient when the search for an optimum happens in an elliptical domain\n",
    "- Gradient-based method are inefficient when the search space solution is large\n",
    "- The choice of a starting point conditions the search and the efficiency of a gradient-based method. Converge towards a local minimum is much likely to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We compute the cost in one large batch computation\n",
    "- When computing the cost function we look at the loss associated with each training example and then sum these values together for an overall cost of the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- Fewer updates to the model means this variant of gradient descent is more computationally efficient than stochastic gradient descent.\n",
    "- The decreased update frequency results in a more stable error gradient and may result in a more stable convergence on some problems.\n",
    "- The separation of the calculation of prediction errors and the model update lends the algorithm to parallel processing based implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cons:\n",
    "- Model updates, and in turn training speed, may become very slow for large datasets.\n",
    "- Commonly, batch gradient descent is implemented in such a way that it requires the entire training dataset in memory and available to the algorithm.\n",
    "- Moreover, it begs the question: do we really need to see all of the data before making improvements to our parameter values?\n",
    "- The updates at the end of the training epoch require the additional complexity of accumulating prediction errors across all training examples.\n",
    "- The more stable error gradient may result in premature convergence of the model to a less optimal set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is the most common implementation of gradient descent used in the field of deep learning.\n",
    "- Mini-batch gradient descent seeks to find a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent.\n",
    "- Allows us to split our training data into mini batches which can be processed individually\n",
    "- Update your parameters accordingly to the processed mini-batch\n",
    "\n",
    "<img width=500 src=\"images/Screen-Shot-2017-11-15-at-7.26.05-PM.png\"/>\n",
    "<center><a href=\"https://www.jeremyjordan.me/gradient-descent/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffling:\n",
    "- Every mini-batch used in the training process should have the same distribution\n",
    "- A difference in the distributions is called the covariate shift\n",
    "- With every GD iteration, randomize the data before creating mini-batches for images to be uniformly sampled from the entire distribution\n",
    "\n",
    "<img width=550 src=\"images/covariate-shift.png\"/>\n",
    "<center><a href=\"https://www.learnopencv.com/batch-normalization-in-deep-networks/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- This allows us to improve our model parameters at each mini batch iteration and take 100 steps towards the global optimum rather than just 1 step\n",
    "- The model update frequency is higher than batch gradient descent which allows for a more robust convergence, avoiding local minima.\n",
    "- Makes use of highly optimized matrix optimizations compared to stochastic gradient descent.\n",
    "- The batching allows both the efficiency of not having all training data in memory and algorithm implementations.\n",
    "- SGD reaches higher validation accuracy than adaptive learning methods while being slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cons:\n",
    "- Requires three loops:\n",
    "  - Over the number of epochs\n",
    "  - Over the number of iterations\n",
    "  - Over the number of layers\n",
    "- Vanilla mini-batch gradient descent, however, does not guarantee good convergence\n",
    "- Introduces a degree of variance into the optimization process\n",
    "- Although we will generally still follow the direction towards the global minimum, we're no longer guarunteed that each step will bring us closer to the optimal parameter values. Advanced optimization techniques are applied here.\n",
    "\n",
    "<img width=400 src=\"images/0*sFYJwQCCjOnXpSoD.png\"/>\n",
    "<center><a href=\"https://hackernoon.com/gradient-descent-aynk-7cbe95a778da\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Mini-batch requires the configuration of an additional “mini-batch size” hyperparameter for the learning algorithm.\n",
    "- Error information must be accumulated across mini-batches of training examples like batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips:\n",
    "- Common batch sizes including: 64, 128, 256, and 512\n",
    "- The presented results confirm that using small batch sizes achieves the best training stability and generalization performance, for a given computational cost, across a wide range of experiments. In all cases the best results have been obtained with batch sizes m = 32 or smaller, often as small as m = 2 or m = 4.\n",
    "- Make sure that a single mini-batch fits into the CPU/GPU memory\n",
    "- Tune batch size and learning rate after tuning all other hyperparameters.\n",
    "- It is a good idea to review learning curves of model validation error against training time with different batch sizes when tuning the batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which optimizer to use:\n",
    "- If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods.\n",
    "- Also, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.\n",
    "- RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances.\n",
    "- Kingma et al. [14:1] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice. However, it is often also worth trying SGD+Nesterov Momentum as an alternative.\n",
    "- Many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. But it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima.\n",
    "\n",
    "<img width=350 src=\"images/saddle_point_evaluation_optimizers.gif\"/>\n",
    "<center><a href=https://giphy.com/gifs/algorithm-nJF1badJjJzW0\"\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When estimating on a small batch, we’re not always going in the optimal direction, because our derivatives are ‘noisy’\n",
    "- To prevent gradient descent from getting stuck at local optima (which may not be the global optimum), we can use a momentum term which allows our search to overcome local optima.\n",
    "- An intuitive understanding of momentum can be painted by a ball rolling down the hill. Its mass is constant all the way, but because of the gravitational pull, its velocity increases over time, making momentum increase. When the directions change, we go marginally slower because our momentum “broke.”\n",
    "- With Momentum update, the parameter vector will build up velocity in any direction that has consistent gradient.\n",
    "\n",
    "<img width=300 src=\"images/momentum.png\"/>\n",
    "<center><a href=\"http://www.junlulocky.com/blog/SGDoverview\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Momentum takes past gradients into account to smooth out the steps of gradient descent\n",
    "- Exponentially weighed averages can provide us a better estimate which is closer to the actual derivate than our noisy calculations\n",
    "\n",
    "<img width=300 src=\"images/1*Fg2t0oqYaFIDx0EXevTy3g.png\"/>\n",
    "\n",
    "GD with momentum | Beta\n",
    ":-:|:-:\n",
    "<img width=300 src=\"images/1*fhHakQ1nWN7HK1KBNdarqw.png\"/> | <img width=300 src=\"images/1*buj-RJg3wW6RSclnpczkzA.png\"/>\n",
    "\n",
    "<center><a href=\"https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- The first couple of iterations will provide a pretty bad averages because we don’t have enough values yet to average over. \n",
    "- The solution is to use the bias-corrected version of velocity\n",
    "\n",
    "<img width=150 src=\"images/1*Ysq5Noczr0Zsuyi-9kzk8A.png\"/>\n",
    "\n",
    "- [Why Momentum Really Works](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- SGD with momentum seems to find more flatter minima than Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips:\n",
    "- The velocity is initialized with zeros. So the algorithm will take a few iterations to \"build up\" velocity and start to take bigger steps.\n",
    "- If $\\beta=0$, then this just becomes standard gradient descent without momentum.\n",
    "- With bigger values of beta, we get much smother curve, but it’s a little bit shifted to the right\n",
    "- Common values for $\\beta$ are in range from 0.8 to 0.999. If you don't feel inclined to tune this, $\\beta=0.9$ is often a reasonable default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSProp tries to dampen the oscillations, but in a different way than momentum\n",
    "- RMSProp implicitly performs simulated annealing. Suppose if we are heading towards the minima, and we want to slow down so as to not to overshoot the minima. RMSProp automatically will decrease the size of the gradient steps towards minima when the steps are too large (Large steps make us prone to overshooting)\n",
    "- While momentum accelerates our search in direction of minima, RMSProp impedes our search in direction of oscillations.\n",
    "\n",
    "<img width=400 src=\"images/1*m_PC8M4y9UKYU9JNuOC9Jw.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- The algorithm does well on online and non-stationary problems (e.g. noisy)\n",
    "- RMSProp is used by DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam or Adaptive Moment Optimization algorithms combines the heuristics of both Momentum and RMSProp\n",
    "- Adam adds bias-correction and momentum to RMSprop.\n",
    "- Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.\n",
    "\n",
    "<img width=300 src=\"images/adam1.png\"/>\n",
    "<img width=150 src=\"images/adam2.png\"/>\n",
    "<img width=300 src=\"images/adam3.png\"/>\n",
    "\n",
    "- [ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION (2017)](https://arxiv.org/pdf/1412.6980.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- In practice, Adam is currently recommended as the default algorithm to use, and often works slightly better than RMSProp.\n",
    "- It has attractive benefits on non-convex optimization problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips:\n",
    "- Good default settings for the tested machine learning problems are $\\alpha=0.001$, $\\beta_1=0.9$, $\\beta_2=0.999$ and $\\epsilon=10^{−8}$. The popular deep learning libraries generally use the default parameters recommended by the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though some optimizers try and use ideas of momentum to have the ability to swing out of a local minimum, they are not always that successful. \n",
    "- We want to encourage our model to find parts of the weight space that are both accurate and stable. Therefore, from time to time we increase the learning rate (this is the 'restarts' in 'SGDR'), which will force the model to jump to a different part of the weight space if the current area is \"spikey\".\n",
    "\n",
    "<img width=400 src=\"images/1_3kkV66xEObjWpYiGdBBivg.png\"/>\n",
    "<center><a href=\"https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- [SGDR: STOCHASTIC GRADIENT DESCENT WITH WARM RESTARTS](https://arxiv.org/pdf/1608.03983.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
