**The clear step-by-step instruction on how to produce the final submit file:**
1. Download competition data and preprocess it with [DataPreparation](DataPreparation.ipynb), which outputs data in a HDF5 format.
2. Run [LightGBM](LightGBM.ipynb) to produce first base-model meta features and predictions. While importing the preprocessed data, pay attention to the structure of folders with input files, since the notebooks were downloaded directly from Kaggle. Similarly run the [CatBoost](CatBoost.ipynb), [LinReg](LinReg.ipynb) and [NeuralNet](NeuralNet.ipynb) notebooks.
4. Run [Stacking](Stacking.ipynb), which takes outputs of the base models and generates CSV files for submission.
5. (optional) For fastest use, upload all notebooks to Kaggle and import the competition data and the respective outputs from other kernels.

**Summary**:
- In our experience, feature engineering was the most challenging but also the most fruitful part of the competition, which was driven by a careful EDA. The split between public and private is random and we found no significant data leakages to play with. The train distribution was adapated to mimic the test distribution. We were able to extend the data by extracting simple features (such as mean encodings, lagged features, names) as well as more complex ones (such as matrix decomposition). For non-tree-based models such as Linear Regression and Neural Networks standard scaling across all features was used. The most relevant features were selected based on the importance scores of the fitted LGBM models. The data was then fitted to Gradient Boosting Models such as LightGBM and CatBoost, Linear Regression (Vowpal Wabbit), and Neural Networks (fastai). On top of them, diverse ensemble methods (such as bagging of LGBM models and blending) were used to compensate for individual model errors.

**Data split and validation scheme:**
- Other than other publicly available kernels, we didn't use the whole dataset nor the fixed number of months prior to the test set: we randomly sampled 3 million records from the whole dataset. This way we reduced the data size but at the same time we didn't loose any information and target distribution remained the same. For example, our train set contains the same number of records from 12st month as from the 32th month; since lagged features are missing in the first couple of months, models generalize better, and so we improved the LB scores. For ensembling we reserved total of 4 months. 
- Since the data contains a time variable, we had to respect it in the validation scheme: we used 33th month as validation set for base models (simple holdout scheme), and a custom validation scheme similar to TimeSeriesSplit for meta models (KFold scheme in time series).

**Data preparation:**
- When exploring the data we identified the discrepancy between training and test distributions. The test set is a cartesian product of shop and item ids within the 34 date block. There are 5100 items * 42 shops = 214200 pairs. To make both sets similar, for each date block in train, we created a product of shops and items which produced a nearly sparse matrix. Furthermore, we aggregated daily sales to monthly sales. We also clipped the target to *[0, 20]* range, which is similarly done by the competition's evaluation mechanism. 
- The preprocessing on text features included several fixes such as duplicate removal, extraction of item types and subtypes, and extraction of the city name. 
- The most influential variables generated were target encodings: we aggregated sales across categorical features and their combinations. We also tried default KFold, expanding, and leave-one-out strategies, but encountered drops in leaderboard scoring, so we skipped them altogether.
- In order for the base models to capture information about past and generalize better, we also introduced a set of lagged features, with small windows being the most benefial. 
- As part of the advanced topics, we utilized matrix decomposition (and dimensionality reduction) in order to generate new features. We processed numeric columns with either PCA or TruncatedSVD (best suited for sparse data), and categorical columns with NMF (best suited for tree-based models). The non-linear transformer t-SNE required too much time and resources so we skipped it. 
- Then we saved the data to the disk for use by other kernels and produced two baseline submissions: 1) global mean and 2) previous month benchmark. Both submissions scored the same LB score as stated by the instructors, giving us the validation for the correctness of the preprocessing steps.
- About RAM optimization: Having millions of records and dozens of features in the dataset we often encountered memory errors, so we downcasted the dataframe to the smallest numeric datatype to safe memory. We'd like to note that *float16* produced strange results since it is less widely supported than *float32*, thus we used the latter.

**LightGBM:**
- LGBM was by far the most powerful and at the same time the fastest model. We used the speed of LGBM to create as many diverse models as possible. Apart from the baseline model with default configuration, we tuned a regressor (whereas we used a smaller subset of data to speed up the process). We then used the tuning history to average models of different configurations. Similarly, we averaged best-performing models with different random seeds, which gave us the best score across LGBMs. Each configuration had the same input, with all categorical features except *item_id* being of type *category*; for features with high cardinality such as *item_id* it often works best to treat them as numeric by simply ignoring the categorical interpretation of the integers.

**CatBoost:**
- Despite that CatBoost often performs better than any other GBM on the market, it does so only when we have categorical variables in the data and we properly tune them. In order to get a feel of which combination of categorical features results in good performance, we tried them all with a brute-force search. Moreover, we saved predictions of each fitted booster to the disk for the blender. As result, we created 8 set of predictions each with slighly worse performance than LGBM.

**Linear regression:**
- Linear models are fully different set of estimators than boosting models and so they require totally different set of preprocessing steps: scaling and encoding. Since the basic implementation of *LogisticRegression* by *sklearn* is slow for our big and high-dimentional data, we used Vowpal Wabbit, which is prominent for its training speed and support of many training modes, especially for online learning (with stochastic GD as a subtype). One important feature of vowpal is hashing trick: a fast and space-efficient way of mapping features to low-dimensional space. Since our *item_id* is changing each month while other ID features are staying constant, we used VW's own hasing algorithm for *item_id* and manually one-hot-encoded other columns. By using this approach, we could get a performance similar to a neural network trained with embeddings, while retaining flexibility. Furthermore, to get a good performance, we tuned the learning rate. The drawback of vowpal is the lack of interpretation: to map feature importance to the columns, we would need to form our features directly beforehand. Vowpal is also memory-hungry, so we used 2/3 of training data.

**Neural network:**
- Having a neural network as a non-linear base regressor is important, and quite simple to implement with *tabular* module of *fastai*. What we did is four different tasks: 1) basic regression using MSE loss, 2) regression using sigmoid within range *[0, 20]* and MSE loss, 3) classification task whether target is 0 or higher with logloss, and 4) the same classification task but with weighted logloss which takes into account imbalanced classes. One could go further and create bumper features, that is, get probability of target being higher than 1, 2, and so on; the only problem is a highly imbalanced dataset. As preprocessing we used scaling for numeric features and fixed-length embeddings for categorical features. Surprisingly, the results of each model were mediocre, despite tuning batch size (in this task higher is better), adapting learning rate, and checking added value of embeddings.

**Ensembling:**
- Overall, the use of blending resulted in 1% performance boost. We used LGBM, linear regression and NN as meta models to learn on outputs of base models. We also included numeric and categorical PCA features into the input. A simpler models such as linear regression performed better than the more complex ones. We also tried some tweaks, such as converting float predictions to integers by knowing target distribution (that 85% of true labels are zeros, 2% are ones, etc.). For this, we considered final predictions as ranks for filling integers (which can also be done by thresholding). This way we could assign zeros to 85% of labels, but unfortunately, the LB scores didn't improve. Furthermore, we did the same for predictions of every base model and then averaged them out, which gave an improvement but still hasn't beaten a basic linear regression.
